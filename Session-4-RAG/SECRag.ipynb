{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "765d1d13-7916-4fc7-8de8-fb2c108267fe",
   "metadata": {},
   "source": [
    "RAG with Milvus Demo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab0c8a90-3ef4-4f44-9845-df9746f821bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain packages\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_huggingface import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad5591ea-a4a8-49f8-a4e1-d0115b4bfffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26bc8307-c8ac-40ca-98ef-4d076e833853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-5.1.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence_transformers)\n",
      "  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: tqdm in /Users/pg47711/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/python3.13/site-packages (from sentence_transformers) (4.67.1)\n",
      "Collecting torch>=1.11.0 (from sentence_transformers)\n",
      "  Using cached torch-2.8.0-cp313-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting scikit-learn (from sentence_transformers)\n",
      "  Using cached scikit_learn-1.7.2-cp313-cp313-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence_transformers)\n",
      "  Using cached scipy-1.16.2-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/pg47711/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/python3.13/site-packages (from sentence_transformers) (0.35.1)\n",
      "Collecting Pillow (from sentence_transformers)\n",
      "  Using cached pillow-11.3.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/pg47711/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/python3.13/site-packages (from sentence_transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /Users/pg47711/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/pg47711/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/pg47711/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/pg47711/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (6.0.3)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading regex-2025.9.18-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /Users/pg47711/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/pg47711/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.22.1)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/pg47711/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/pg47711/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.1.10)\n",
      "Requirement already satisfied: setuptools in /Users/pg47711/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (80.9.0)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/pg47711/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/pg47711/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/python3.13/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/pg47711/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pg47711/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pg47711/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pg47711/homebrew/opt/certifi/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (2025.8.3)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence_transformers)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence_transformers)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading sentence_transformers-5.1.1-py3-none-any.whl (486 kB)\n",
      "Downloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.9.18-cp313-cp313-macosx_11_0_arm64.whl (287 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl (432 kB)\n",
      "Using cached torch-2.8.0-cp313-none-macosx_11_0_arm64.whl (73.6 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached pillow-11.3.0-cp313-cp313-macosx_11_0_arm64.whl (4.7 MB)\n",
      "Using cached scikit_learn-1.7.2-cp313-cp313-macosx_12_0_arm64.whl (8.6 MB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached scipy-1.16.2-cp313-cp313-macosx_14_0_arm64.whl (20.9 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: mpmath, threadpoolctl, sympy, scipy, safetensors, regex, Pillow, networkx, joblib, torch, scikit-learn, transformers, sentence_transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/13\u001b[0m [sentence_transformers]ence_transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed Pillow-11.3.0 joblib-1.5.2 mpmath-1.3.0 networkx-3.5 regex-2025.9.18 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.2 sentence_transformers-5.1.1 sympy-1.14.0 threadpoolctl-3.6.0 torch-2.8.0 transformers-4.56.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fff706e-238d-44ca-9de7-8d7dbb740fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents we want to prompt an LLM about\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbea2c5-c16a-4cdd-9cc5-60772e7769c9",
   "metadata": {},
   "source": [
    "# Step 1 : Data Loading\n",
    "## Connect to the source of data.\n",
    "## Extract text from the file.\n",
    "## Review and update metadata information.\n",
    "## Clean or transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f0c579e-5774-4878-8158-d0c0d1998b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1037 pages from 20 files\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "\n",
    "FOLDER_PATH = \"/Users/pg47711/RAG Demo/docs/\"\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    FOLDER_PATH,\n",
    "    glob=\"**/*.pdf\",           # use \"*.pdf\" for non-recursive\n",
    "    loader_cls=PyPDFLoader,\n",
    "    use_multithreading=True,   # speeds up loading\n",
    ")\n",
    "docs = loader.load()\n",
    "print(f\"✅ Loaded {len(docs)} pages from {len(set(d.metadata['source'] for d in docs))} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932d94af-9c1c-40a9-bea9-111db6b4c20f",
   "metadata": {},
   "source": [
    "## Extract more MetaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85d70878-487d-41c7-a684-6aeb968fcc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_year': 2023, 'file_quarter': 'Q1', 'company': 'AMZN'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "MONTH_TO_Q = {\n",
    "    \"jan\": 1, \"january\": 1,\n",
    "    \"feb\": 1, \"february\": 1,\n",
    "    \"mar\": 1, \"march\": 1,\n",
    "    \"apr\": 2, \"april\": 2,\n",
    "    \"may\": 2,\n",
    "    \"jun\": 2, \"june\": 2,\n",
    "    \"jul\": 3, \"july\": 3,\n",
    "    \"aug\": 3, \"august\": 3,\n",
    "    \"sep\": 3, \"sept\": 3, \"september\": 3,\n",
    "    \"oct\": 4, \"october\": 4,\n",
    "    \"nov\": 4, \"november\": 4,\n",
    "    \"dec\": 4, \"december\": 4,\n",
    "}\n",
    "\n",
    "def parse_filename_fields(source_path: str):\n",
    "    \"\"\"\n",
    "    Extract year, quarter, and company/ticker from PDF filename.\n",
    "    Examples handled:\n",
    "      - '2023 Q1 AMZN.pdf'\n",
    "      - 'AMZN_Q1_2023.pdf'\n",
    "      - '2023-Q2-MSFT 10-Q.pdf'\n",
    "      - 'Q3 2024 Google.pdf'\n",
    "      - 'Amazon 2023Q1.pdf'\n",
    "      - '2024-04-30 Meta Q2.pdf' (quarter deduced from month if needed)\n",
    "    \"\"\"\n",
    "    fn = os.path.basename(source_path or \"\")\n",
    "    name, _ = os.path.splitext(fn)\n",
    "    # Normalize separators to spaces\n",
    "    s = re.sub(r\"[_\\-]+\", \" \", name)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    # Year\n",
    "    year = None\n",
    "    m_year = re.search(r\"\\b(19|20)\\d{2}\\b\", s)\n",
    "    if m_year:\n",
    "        year = int(m_year.group(0))\n",
    "\n",
    "    # Quarter (Q1/Q 1/Quarter 1/1Q)\n",
    "    quarter = None\n",
    "    # Try Q patterns first\n",
    "    m_q = re.search(r\"\\bQ[ \\-]?([1-4])\\b\", s, flags=re.IGNORECASE) or \\\n",
    "          re.search(r\"\\b([1-4])[ \\-]?Q\\b\", s, flags=re.IGNORECASE) or \\\n",
    "          re.search(r\"\\bQuarter[ \\-]?([1-4])\\b\", s, flags=re.IGNORECASE)\n",
    "    if m_q:\n",
    "        quarter = f\"Q{m_q.group(1)}\"\n",
    "    else:\n",
    "        # Fallback: infer from month mention\n",
    "        for mon, q in MONTH_TO_Q.items():\n",
    "            if re.search(rf\"\\b{mon}\\b\", s, flags=re.IGNORECASE):\n",
    "                quarter = f\"Q{q}\"\n",
    "                break\n",
    "\n",
    "    # Remove tokens (year/quarter/common noise) to isolate company\n",
    "    rem = s\n",
    "    if m_year:\n",
    "        rem = re.sub(rf\"\\b{m_year.group(0)}\\b\", \" \", rem)\n",
    "    rem = re.sub(r\"\\bQ[ \\-]?[1-4]\\b|\\b[1-4][ \\-]?Q\\b|\\bQuarter[ \\-]?[1-4]\\b\",\n",
    "                 \" \", rem, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove common report terms\n",
    "    noise = [\n",
    "        \"10-Q\",\"10K\",\"10-K\",\"Form\",\"Report\",\"Earnings\",\"Quarterly\",\"Annual\",\n",
    "        \"Statement\",\"Results\",\"Filing\"\n",
    "    ]\n",
    "    for n in noise:\n",
    "        rem = re.sub(rf\"\\b{re.escape(n)}\\b\", \" \", rem, flags=re.IGNORECASE)\n",
    "\n",
    "    company = re.sub(r\"\\s+\", \" \", rem).strip()\n",
    "\n",
    "    # If company is still empty but filename contained obvious ticker at end like '(AMZN)'\n",
    "    if not company and s:\n",
    "        company = s  # fallback to the sanitized name\n",
    "\n",
    "    return {\n",
    "        \"file_year\": year or 0,\n",
    "        \"file_quarter\": quarter or \"\",\n",
    "        \"company\": company or \"\",\n",
    "    }\n",
    "\n",
    "# Quick test\n",
    "parse_filename_fields(\"/Users/pg47711/RAG Demo/docs/2023 Q1 AMZN.pdf\")\n",
    "# -> {'file_year': 2023, 'file_quarter': 'Q1', 'company': 'AMZN'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bae7a16-78c9-4979-b377-1c1673c4d6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def normalize_pdf_metadata(md: dict) -> dict:\n",
    "    source = md.get(\"source\") or \"\"\n",
    "    filename = os.path.basename(source) if source else \"\"\n",
    "\n",
    "    # Parse fields from filename\n",
    "    name_fields = parse_filename_fields(source)\n",
    "    file_year = int(name_fields[\"file_year\"] or 0)\n",
    "    file_quarter = name_fields[\"file_quarter\"] or \"\"\n",
    "    company = name_fields[\"company\"] or \"\"\n",
    "\n",
    "    # Existing creation date parsing\n",
    "    creation_raw = md.get(\"creationdate\") or md.get(\"CreationDate\") or \"\"\n",
    "    creation_iso, creation_ts, year_meta, quarter_meta = \"\", 0, 0, \"\"\n",
    "\n",
    "    if creation_raw:\n",
    "        try:\n",
    "            dt = datetime.fromisoformat(creation_raw.replace(\"Z\", \"+00:00\"))\n",
    "            creation_iso = dt.isoformat()\n",
    "            creation_ts = int(dt.timestamp())\n",
    "            year_meta = dt.year\n",
    "            quarter_meta = f\"Q{(dt.month - 1)//3 + 1}\"\n",
    "        except Exception:\n",
    "            creation_iso = str(creation_raw)\n",
    "\n",
    "    # Choose a canonical year/quarter: prefer filename if present, else metadata\n",
    "    canonical_year = file_year or year_meta or 0\n",
    "    canonical_quarter = file_quarter or quarter_meta or \"\"\n",
    "\n",
    "    return {\n",
    "        # original metadata\n",
    "        \"source\": source,\n",
    "        \"filename\": filename,\n",
    "        \"title\": str(md.get(\"title\") or \"\"),\n",
    "        \"author\": str(md.get(\"author\") or \"\"),\n",
    "        \"creator\": str(md.get(\"creator\") or \"\"),\n",
    "        \"producer\": str(md.get(\"producer\") or \"\"),\n",
    "        \"subject\": str(md.get(\"subject\") or \"\"),\n",
    "        \"keywords\": str(md.get(\"keywords\") or \"\"),\n",
    "        \"total_pages\": int(md.get(\"total_pages\") or 0),\n",
    "        \"page\": int(md.get(\"page\") or -1),\n",
    "        \"page_label\": str(md.get(\"page_label\") or \"\"),\n",
    "        \"creationdate\": creation_iso,\n",
    "        \"creation_ts\": int(creation_ts),\n",
    "\n",
    "        # derived fields (filter-friendly)\n",
    "        \"company\": company,\n",
    "        \"file_year\": file_year,\n",
    "        \"file_quarter\": file_quarter,\n",
    "\n",
    "        # canonical fields to filter on\n",
    "        \"year\": canonical_year,\n",
    "        \"quarter\": canonical_quarter,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce38fa4c-cdf5-407f-a865-aad819d2eaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample normalized metadata: {'source': '/Users/pg47711/RAG Demo/docs/2023 Q1 AMZN.pdf', 'filename': '2023 Q1 AMZN.pdf', 'title': '0001018724-23-000008', 'author': 'EDGAR Online, a division of Donnelley Financial Solutions', 'creator': 'EDGAR Filing HTML Converter', 'producer': 'EDGRpdf Service w/ EO.Pdf 22.0.40.0', 'subject': 'Form 10-Q filed on 2023-04-28 for the period ending 2023-03-31', 'keywords': '0001018724-23-000008; ; 10-Q', 'total_pages': 52, 'page': 15, 'page_label': '16', 'creationdate': '2023-04-28T06:12:55-04:00', 'creation_ts': 1682676775, 'company': 'AMZN', 'file_year': 2023, 'file_quarter': 'Q1', 'year': 2023, 'quarter': 'Q1'}\n"
     ]
    }
   ],
   "source": [
    "# docs: list of langchain.schema.Document\n",
    "metas = [normalize_pdf_metadata(d.metadata) for d in docs]\n",
    "\n",
    "# Optional: text content to store alongside metadata\n",
    "texts = [d.page_content for d in docs]\n",
    "\n",
    "# Quick peek\n",
    "print(\"Sample normalized metadata:\", metas[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64690b13-8a82-4144-b47f-2233ca07f7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column arrays aligned to your schema (adjust order to your schema)\n",
    "text_col        = texts\n",
    "source_col      = [m[\"source\"] for m in metas]\n",
    "filename_col    = [m[\"filename\"] for m in metas]\n",
    "title_col       = [m[\"title\"] for m in metas]\n",
    "author_col      = [m[\"author\"] for m in metas]\n",
    "creator_col     = [m[\"creator\"] for m in metas]\n",
    "producer_col    = [m[\"producer\"] for m in metas]\n",
    "subject_col     = [m[\"subject\"] for m in metas]\n",
    "keywords_col    = [m[\"keywords\"] for m in metas]\n",
    "total_pages_col = [m[\"total_pages\"] for m in metas]\n",
    "page_col        = [m[\"page\"] for m in metas]\n",
    "page_label_col  = [m[\"page_label\"] for m in metas]\n",
    "creationdate_col= [m[\"creationdate\"] for m in metas]\n",
    "creation_ts_col = [m[\"creation_ts\"] for m in metas]\n",
    "company_col     = [m[\"company\"] for m in metas]\n",
    "file_year_col   = [m[\"file_year\"] for m in metas]\n",
    "file_quarter_col= [m[\"file_quarter\"] for m in metas]\n",
    "year_col        = [m[\"year\"] for m in metas]\n",
    "quarter_col     = [m[\"quarter\"] for m in metas]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7b8434-2c2f-4e80-96f4-6b89b7633784",
   "metadata": {},
   "source": [
    "## Create Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc1824e9-ef6d-443f-be72-5a8ded296516",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 33/33 [00:01<00:00, 16.74it/s]\n"
     ]
    }
   ],
   "source": [
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"  # 384-dim, fast and solid\n",
    "embedder = SentenceTransformer(EMBED_MODEL)\n",
    "DIM = embedder.get_sentence_embedding_dimension()\n",
    "\n",
    "# Note: normalize_embeddings=True pairs well with COSINE\n",
    "vectors = embedder.encode(text_col, normalize_embeddings=True, show_progress_bar=True).tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c01f0c3-ba12-4c72-9a18-bb8446e56ba1",
   "metadata": {},
   "source": [
    "## Milvus Load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e20f44b-4614-4274-ad2f-a7826f81446e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inserted entities: 1037\n",
      "✅ Collection loaded for search\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "MILVUS_HOST = \"localhost\"  # If your endpoint is literally 'local_host', change this accordingly\n",
    "MILVUS_PORT = \"19530\"\n",
    "COLLECTION_NAME = \"sec_filings\"\n",
    "\n",
    "connections.connect(\"default\", host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "\n",
    "# Optional: drop collection if it exists (for clean reruns)\n",
    "if utility.has_collection(COLLECTION_NAME):\n",
    "    utility.drop_collection(COLLECTION_NAME)\n",
    "\n",
    "# 4) Define schema (make sure field names match your columns)\n",
    "# Note: VARCHAR requires max_length; adjust as you prefer\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "\n",
    "    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=DIM),\n",
    "\n",
    "    FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=8192),\n",
    "    FieldSchema(name=\"source\", dtype=DataType.VARCHAR, max_length=1024),\n",
    "    FieldSchema(name=\"filename\", dtype=DataType.VARCHAR, max_length=512),\n",
    "\n",
    "    FieldSchema(name=\"title\", dtype=DataType.VARCHAR, max_length=512),\n",
    "    FieldSchema(name=\"author\", dtype=DataType.VARCHAR, max_length=512),\n",
    "    FieldSchema(name=\"creator\", dtype=DataType.VARCHAR, max_length=512),\n",
    "    FieldSchema(name=\"producer\", dtype=DataType.VARCHAR, max_length=512),\n",
    "    FieldSchema(name=\"subject\", dtype=DataType.VARCHAR, max_length=512),\n",
    "    FieldSchema(name=\"keywords\", dtype=DataType.VARCHAR, max_length=1024),\n",
    "\n",
    "    FieldSchema(name=\"total_pages\", dtype=DataType.INT64),\n",
    "    FieldSchema(name=\"page\", dtype=DataType.INT64),\n",
    "    FieldSchema(name=\"page_label\", dtype=DataType.VARCHAR, max_length=64),\n",
    "\n",
    "    FieldSchema(name=\"creationdate\", dtype=DataType.VARCHAR, max_length=64),\n",
    "    FieldSchema(name=\"creation_ts\", dtype=DataType.INT64),\n",
    "\n",
    "    # Filename-derived and canonical filters\n",
    "    FieldSchema(name=\"company\", dtype=DataType.VARCHAR, max_length=128),\n",
    "    FieldSchema(name=\"file_year\", dtype=DataType.INT64),\n",
    "    FieldSchema(name=\"file_quarter\", dtype=DataType.VARCHAR, max_length=8),\n",
    "    FieldSchema(name=\"year\", dtype=DataType.INT64),\n",
    "    FieldSchema(name=\"quarter\", dtype=DataType.VARCHAR, max_length=8),\n",
    "]\n",
    "\n",
    "schema = CollectionSchema(fields, description=\"Sec filings pages with text, embeddings, and rich metadata\")\n",
    "col = Collection(name=COLLECTION_NAME, schema=schema)\n",
    "\n",
    "# 5) Create a vector index (COSINE works best with normalized embeddings)\n",
    "index_params = {\n",
    "    \"metric_type\": \"IP\",\n",
    "    \"index_type\": \"IVF_FLAT\",  # for larger scale consider HNSW or IVF_SQ8\n",
    "    \"params\": {\"nlist\": 1024},\n",
    "}\n",
    "col.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "\n",
    "# 6) Prepare data and insert (order must match schema after the auto-id primary key)\n",
    "# Ensure text length <= max_length in schema\n",
    "text_col_trunc = [t[:8192] if isinstance(t, str) else \"\" for t in text_col]\n",
    "\n",
    "# Sanity check: all column lengths must match\n",
    "n = len(vectors)\n",
    "assert all(len(lst) == n for lst in [\n",
    "    text_col_trunc, source_col, filename_col, title_col, author_col, creator_col, producer_col,\n",
    "    subject_col, keywords_col, total_pages_col, page_col, page_label_col, creationdate_col,\n",
    "    creation_ts_col, company_col, file_year_col, file_quarter_col, year_col, quarter_col\n",
    "]), \"Column length mismatch — check your input lists.\"\n",
    "\n",
    "# Insert column-wise (skip auto-id field)\n",
    "mr = col.insert([\n",
    "    vectors,\n",
    "    text_col_trunc,\n",
    "    source_col,\n",
    "    filename_col,\n",
    "    title_col,\n",
    "    author_col,\n",
    "    creator_col,\n",
    "    producer_col,\n",
    "    subject_col,\n",
    "    keywords_col,\n",
    "    total_pages_col,\n",
    "    page_col,\n",
    "    page_label_col,\n",
    "    creationdate_col,\n",
    "    creation_ts_col,\n",
    "    company_col,\n",
    "    file_year_col,\n",
    "    file_quarter_col,\n",
    "    year_col,\n",
    "    quarter_col,\n",
    "])\n",
    "col.flush()\n",
    "print(f\"✅ Inserted entities: {mr.insert_count}\")\n",
    "\n",
    "# 7) Load collection for search\n",
    "col.load()\n",
    "print(\"✅ Collection loaded for search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7441c5e-8500-4a07-a55b-d1e91ef8e966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
